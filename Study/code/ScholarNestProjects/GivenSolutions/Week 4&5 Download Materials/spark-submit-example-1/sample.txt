Welcome Back.
In this video, I will talk about Spark Dataframes.
So let's start.

We learned that Spark is a data processing platform. Right?
What other data processing platform do you know?
Databases? Right?
Databases are the most popular and most widely used data processing platforms.
And what do they offer?
They offer two things at a high level.
1. Tables
2. SQL

A database table allows you to load the data in the table.
The data in a table is internally stored as a .dbf file.
And these "dbf" files are stored on the disk.
But you don't care about the "dbf" file and the storage layer.
You only care about the table. Right?
You always look at the table and query the table. Right?
But what is a table?
A table contains two things.
1. Table Schema
2. Table Data
What is a table schema?
Simple! It is a list of column names and data types. Right?
The schema information is stored in a database data dictionary or a metadata store.
This is how a table is organized.
We have three layers to form a table.
1. Storage layer stores the table data in a file
2. Metadata layer stores the table schema and other important information
3. The Logical layer presents you with a database table, and you can execute SQL queries on the logical table.
Make sense?
Great! 
NOw let's see how SQL executes.
When you submit a SQL query to your database SQL Engine, the database will refer to the metadata store for parsing your SQL queries.
And the database will throw a syntax error or an analysis error if you are using a column name in your SQL that does not exist in the metadata store.
Here is an example of SQL.
This SQL will return an analysis exception because the column XYZ does not exist in the table.

So the schema is essential for the database table and for the SQL expressions to work correctly. Right?
The table data is stored in the dbf files behind the table.
So if your SQL query is correct and passes all the schema validation, you will see query results.
The database will read data from the "dbf" file, process it according to your SQL query, and show you the results.
Make sense?
That is all about the data processing in databases.

Now let's try database analogy with Apache Spark.
Apache Spark offers you two ways of data processing.
1. Spark Database and SQL
2. Spark Dataframe and Dataframe API

The first approach is precisely the same as a typical database.
So you will create table and load data into the table.
Spark table data is internally stored in the data files.
But these files are not dbf files.
Spark gives you the flexibility to choose the file format and supports many file formats such as the following.
1. CSV
2. JSON
3. Parquet
4. AVRO
5. XML

And that is the reason. Spark supports structured, semi-structured, and unstructured data.
The DBF file format and database engine were designed to support structured data.
But Spark is designed to support a variety of file formats, and hence you can store and process a variety of data formats. Make sense?
The spark storage layer also supports distributed storage such as HDFS and Cloud storage such as Amazon S3 and Azure ADLS.
So you are not limited to disk storage capacity. You can use distributed storage and store large data files.
Make sense? 
Great! Spark also has a metadata store for storing table schema information. So that part is similar to the databases.
Then Spark also comes with an SQL query engine and supports standard SQL syntax for processing and querying data from Spark tables.

However, Spark goes beyond the Tables and SQL to offer Spark Dataframe and Dataframe API.
So what is a Spark Dataframe?
It is the same as the table without a metadata store.
What does it mean?
Simple! Spark Dataframe is structurally the same as the table.
However, it does not store any schema information in the metadata store.
Instead, we have a runtime metadata catalog to store the dataframe schema information.
The catalog is similar to the metadata store, but Spark will create it at the runtime to store schema information in the catalog.
This catalog is only valid until your application is running.
Spark will delete this catalog when your Spark application terminates.
What is the benefit? Why not store the Dataframe schema in the metadata store?
Why create a runtime metadata catalog?
Well, we have two reasons for this.
1. Stark Dataframe is a runtime object.
2. Spark Dataframe supports schema-on-read.
What does it mean?
Simple! You can create a Spark Data frame at runtime and keep it in memory until your program terminates.
Once your program terminates, your dataframe is gone.
It is an in-memory object. Spark tables are permanent.
Once created, you will have a table forever. You can drop a table and remove it.
But it remains in the system until you drop the table.

However, Spark Dataframe is a runtime and temporary object which lives in Spark memory and goes away when the application terminates.
So the metadata is also stored in the temporary metadata catalog.
Make sense?

Great! The second reason is due to the schema-on-read feature.
Spark Dataframe is designed to support the idea of schema-on-read.
What does it mean?
Dataframe does not have a fixed and predefined schema stored in the metadata store.
Instead, we define the schema when we want to read the data from a file and load it into the Dataframe.
So the difference is straightforward.
We define a schema for the table when creating a table.
Then we load data into the table. The data must comply with the table schema, or you will get an error.

However, Dataframe is different.
We load the data into a Dataframe and tell the schema when loading the data.
And Spark will read the file, apply the schema at the time of reading, create the Dataframe using the schema and load the data.
So a Dataframe is always loaded with some data, whereas a Table can be empty. Make sense?

You will see all this happening when we start coding.
So do not worry much if you do not get the idea of schema-on-read.
I will show you that with an example.


Great! That's all about Table vs. Dataframe.
You can use SQL on the table.
However, Dataframe does not support SQL expressions.
You must use Dataframe APIs to process data from a Dataframe.

Since the table and dataframe are structurally the same, you can convert them to each other.
I mean, you can convert a table into a dataframe and vice versa.
The point is straight.
You have two ways of processing data in Spark.
1. SQL
2. Dataframe API
And you can use both at your convenience.
You can create a table and use SQL or convert a table into a Dataframe and use Dataframe API on the same table.
We will learn all thatâ€”nothing to worry about.

But for now, we learned some basics about Spark Tables and Dataframes.
I hope you understand both of these objects.
Let me quickly summarize the difference.

1. Tables store schema information in the metadata store
1. Dataframe stores schema information in runtime catalog

2. Table and metadata stores are persistent objects and visible across applications
2. Dataframe and Catalog are runtime objects and live only during the application runtime

3. We create tables with a predefined table schema
3. Dataframe supports schema-on-read

4. Table supports SQL Expressions and does not support API
4. Dataframe offers APIs and does not support SQL expressions 

A Spark table and a Dataframe are convertible objects.
So you can convert a table into Dataframe and save a Dataframe as a Spark table.

And that is all for this lecture.
In the following video, we will do some hands-on activities to implement Spark Tables and Dataframes.
We will learn to use both. So you clearly understand these concepts.
See you again.
Keep Learning and Keep Growing.